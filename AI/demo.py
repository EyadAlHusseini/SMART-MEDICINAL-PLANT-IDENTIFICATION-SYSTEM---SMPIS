# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xmdq9Sk4O9L1Lm0RfY8D5ozDoZSGfSdD

# libraries
"""

# necessary dependencies
from google.colab import drive
import numpy as np
import pandas as pd
import os
import random

# data processing
from PIL import Image, ImageOps
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.applications.vgg19 import preprocess_input

# model handling
from tensorflow.keras.applications import VGG19
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split

# performance measurement
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""# getting the data"""

drive.mount('/content/drive')

root = "/content/drive/MyDrive/Colab Notebooks/grad proj/data/Augmented Images"
imgexts = {".jpg", ".jpeg", ".png"}

"""
getting the data from the folders...
...as paths and label
"""
data = []

MAX_PER_CLASS = 490

for folder in os.listdir(root):
    folderpath = os.path.join(root, folder)
    if not os.path.isdir(folderpath):
        continue

    images = [
        f for f in os.listdir(folderpath)
        if os.path.splitext(f)[1].lower() in imgexts
    ]

    # randomize order
    random.shuffle(images)

    # cap at 490 per folder
    images = images[:MAX_PER_CLASS]

    print(folder, "has", len(images), "images (capped)")

    for imgname in images:
        imgpath = os.path.join(folderpath, imgname)
        data.append((imgpath, folder))

print("Total loaded:", len(data))

"""### truning it into a dataframe"""

df = pd.DataFrame(data,columns=["paths", "label"])

df.sample(5)

"""
df[x]['label'] =  Mint Leaf --> df[x]['label'] =  Mint Leaf  = 4
"""
le = LabelEncoder()
df['labelencoded'] = le.fit_transform(df['label'])

df.sample(5)

"""# pre-processing the data

note: this cell takes a lot of time
"""

"""
We go through the entire dataframe:
1- Making sure the images are RGB.
2- We use ImageOps to make sure all images are the same size.
3- We convert the images into a NumPy array.
4- Apply VGG19 preprocessing to x.
"""

target_size = (224, 224)

x = []
y = []

for row in df.itertuples(index=False):
    img = Image.open(row.paths).convert("RGB")
    img = ImageOps.pad(img, target_size)

    x.append(np.asarray(img, dtype=np.float32))
    y.append(row.labelencoded)

x = np.asarray(x, dtype=np.float32)
y = np.asarray(y)

x = preprocess_input(x)

print(x.shape,y.shape)
print(x.dtype)

"""### splting the data"""

xtrain, xtest,ytrain,ytest = train_test_split(
    x,y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

print(xtrain.shape,ytrain.shape)
print(xtest.shape,ytest.shape)

"""# model time

### initializing the backbone
"""

basemodel = VGG19(
    # weights = imagenet
    include_top = False,
    input_shape = (224,224,3)
)

"""
freezing the layers of the model(VGG19) itself
"""

for layer in basemodel.layers:
  layer.trainable = False

"""### making the classification head"""

x = basemodel.output # the output of the model
x = Flatten()(x) # flatten so we can train a layer on our data
x = Dense(256,activation='relu')(x) # said layer
output = Dense(6,activation='softmax')(x) # the class classifier

"""### compiling the final model"""

# attaching the classification head
model = Model(inputs = basemodel.input,outputs=output)

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

"""# the Magnum opus"""

history = model.fit(
    xtrain,ytrain,
    validation_data = (xtest,ytest),
    batch_size=32,
    epochs=10
)

"""# performance measurement"""

ypred_probs = model.predict(xtest)

ypred = np.argmax(ypred_probs, axis=1)

acc = accuracy_score(ytest, ypred)
print("Validation accuracy:", acc)

print(classification_report(
    ytest,
    ypred,
    target_names=le.classes_
))

"""# deployment"""

model.save('leaf_model.keras')